# Langchain-Toutorial

# Complete LangChain Tutorial for Beginners (2025 Edition)

## Table of Contents
1. [What is LangChain?](#what-is-langchain)
2. [Installation & Setup](#installation--setup)
3. [Core Concepts](#core-concepts)
4. [Hands-On Examples](#hands-on-examples)
5. [Advanced Topics](#advanced-topics)

---

## What is LangChain?

**LangChain** is a framework that makes it easy to build applications powered by Large Language Models (LLMs) like GPT-4, Claude, or Llama.

### Why Use LangChain?
- **Simplifies LLM Integration**: Works with any LLM provider (OpenAI, Anthropic, etc.)
- **Connects to External Data**: Your LLM can access documents, databases, APIs
- **Memory Management**: Chatbots can remember conversation history
- **Tool Integration**: LLMs can use tools like calculators, web search, etc.

### Real-World Use Cases
- Customer support chatbots
- Document question-answering systems
- Data analysis assistants
- Content generation tools
- Research assistants

---

## Installation & Setup

### Step 1: Install Python
Make sure you have **Python 3.8+** installed:
```bash
python --version
```

### Step 2: Create a Virtual Environment (Recommended)
```bash
# Create a new folder for your project
mkdir langchain_project
cd langchain_project

# Create virtual environment
python -m venv venv

# Activate it
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
source venv/bin/activate
```

### Step 3: Install LangChain
```bash
# Core LangChain library
pip install langchain

# For OpenAI models
pip install langchain-openai

# For document loading
pip install langchain-community

# For embeddings and vector stores
pip install langchain-chroma
```

### Step 4: Get API Keys
You'll need an API key from an LLM provider:

**OpenAI (Most Popular)**:
1. Go to [platform.openai.com](https://platform.openai.com)
2. Sign up and navigate to API keys
3. Create a new secret key

**Store your API key securely**:
```bash
# On Windows (Command Prompt):
set OPENAI_API_KEY=your-key-here

# On Mac/Linux (Terminal):
export OPENAI_API_KEY=your-key-here
```

Or use a `.env` file (recommended):
```bash
pip install python-dotenv
```

Create a `.env` file in your project:
```
OPENAI_API_KEY=your-key-here
```

---

## Core Concepts

### 1. **Models**: The Brain
Models are the LLMs that understand and generate text.

### 2. **Prompts**: Instructions to Models
Prompts tell the model what to do.

### 3. **Chains**: Sequential Operations
Chains connect multiple steps together.

### 4. **Memory**: Remembering Context
⚠️ **IMPORTANT (2025 Update)**: As of LangChain v0.3, the recommended approach for adding memory is using LangGraph persistence instead of the old ConversationChain and BufferMemory classes. The old memory classes still work but are no longer recommended for new applications.

**Modern approach (2025)**:
- Use **LangGraph** with `MemorySaver` checkpointer
- Use **thread_id** to manage multiple conversations
- More flexible and production-ready

**Old approach (deprecated)**:
- ConversationChain, ConversationBufferMemory
- Limited to single conversations
- Still works but not recommended

### 5. **Tools/Agents**: Taking Actions
Agents can use tools to perform tasks like searching the web or doing calculations.

### 6. **Document Loaders**: Reading Data
Document loaders help you read PDFs, websites, databases, etc.

### 7. **Embeddings & Vector Stores**: Semantic Search
Embeddings convert text into numbers for similarity search.

---

## Hands-On Examples

### Example 1: Your First LangChain Program
**What it does**: Simple question-answering with an LLM

```python
# Import necessary libraries
from langchain_openai import ChatOpenAI  # For OpenAI's chat models
from dotenv import load_dotenv  # For loading environment variables

# Load environment variables from .env file
load_dotenv()

# Initialize the LLM
# ChatOpenAI creates a connection to OpenAI's GPT models
# model="gpt-4o-mini" specifies which model to use (gpt-4o-mini is cost-effective)
# temperature=0 means responses will be more deterministic (less random)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Invoke the model with a simple question
# The invoke() method sends your prompt to the LLM and gets a response
response = llm.invoke("What is LangChain in one sentence?")

# Print the response
# response.content contains the text generated by the LLM
print(response.content)
```

**Expected Output**:
```
LangChain is an open-source framework that simplifies building applications powered by large language models.
```

---

### Example 2: Using Prompt Templates
**What it does**: Reusable prompts with variables

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate  # For creating prompt templates
from dotenv import load_dotenv

load_dotenv()

# Create a prompt template
# This is a reusable template with placeholders {topic} and {language}
template = """You are a helpful assistant that explains concepts simply.
Explain {topic} to a beginner in {language}."""

# ChatPromptTemplate converts our string template into a LangChain prompt
prompt = ChatPromptTemplate.from_template(template)

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# Format the prompt with actual values
# This replaces {topic} with "machine learning" and {language} with "English"
formatted_prompt = prompt.format(topic="machine learning", language="English")

# Get response
response = llm.invoke(formatted_prompt)
print(response.content)
```

**Why this is useful**: You can reuse the same template with different topics and languages!

---

### Example 3: Creating a Chain (Combining Steps)
**What it does**: Chains connect prompts and models together

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser  # Converts response to string
from dotenv import load_dotenv

load_dotenv()

# Step 1: Create a prompt template
template = "Tell me a fun fact about {subject}"
prompt = ChatPromptTemplate.from_template(template)

# Step 2: Initialize the model
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# Step 3: Create an output parser
# This converts the LLM's response object into a simple string
output_parser = StrOutputParser()

# Step 4: Create a chain by connecting components with the | operator
# The | operator pipes the output of one step into the next
# Flow: prompt -> LLM -> output_parser
chain = prompt | llm | output_parser

# Step 5: Execute the chain
# invoke() runs the entire chain with the provided input
result = chain.invoke({"subject": "space"})
print(result)
```

**How it works**:
1. Prompt gets formatted with "space"
2. Formatted prompt goes to LLM
3. LLM generates response
4. Output parser converts response to string

---

### Example 4: Chatbot with Memory (2025 Modern Approach)
**What it does**: A chatbot that remembers previous messages using LangGraph

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.checkpoint.memory import MemorySaver  # NEW: LangGraph's memory system
from langgraph.graph import START, MessagesState, StateGraph  # NEW: Graph-based approach
from dotenv import load_dotenv

load_dotenv()

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# Create a StateGraph workflow
# MessagesState is a built-in state that handles message history automatically
workflow = StateGraph(state_schema=MessagesState)

# Define the function that calls the model
def call_model(state: MessagesState):
    """
    This function is called at each step of the conversation.
    state: Contains all messages in the conversation
    """
    # Create a system prompt (instructions for the AI)
    system_prompt = (
        "You are a helpful assistant. "
        "Answer all questions to the best of your ability."
    )
    
    # Combine system message with conversation history
    # state["messages"] contains all previous messages automatically
    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    
    # Get response from the LLM
    response = llm.invoke(messages)
    
    # Return the response - LangGraph will automatically add it to state
    return {"messages": response}

# Add the model-calling function as a node in the graph
workflow.add_node("model", call_model)

# Connect the START to our model node
workflow.add_edge(START, "model")

# Create a memory saver (checkpointer)
# This stores conversation history between turns
memory = MemorySaver()

# Compile the graph with memory
# The checkpointer enables memory/persistence
app = workflow.compile(checkpointer=memory)

# Now let's have a conversation!
# Each conversation needs a unique thread_id to track history

# First message
response1 = app.invoke(
    {"messages": [HumanMessage(content="My name is Alice")]},
    config={"configurable": {"thread_id": "conversation-1"}}  # Unique conversation ID
)
print(f"User: My name is Alice")
print(f"Assistant: {response1['messages'][-1].content}\n")

# Second message - it should remember your name!
response2 = app.invoke(
    {"messages": [HumanMessage(content="What's my name?")]},
    config={"configurable": {"thread_id": "conversation-1"}}  # Same thread_id!
)
print(f"User: What's my name?")
print(f"Assistant: {response2['messages'][-1].content}\n")

# Third message
response3 = app.invoke(
    {"messages": [HumanMessage(content="What have we talked about so far?")]},
    config={"configurable": {"thread_id": "conversation-1"}}  # Same thread_id!
)
print(f"User: What have we talked about so far?")
print(f"Assistant: {response3['messages'][-1].content}")

# NEW CONVERSATION (different thread_id) - it won't remember Alice!
response4 = app.invoke(
    {"messages": [HumanMessage(content="Do you know my name?")]},
    config={"configurable": {"thread_id": "conversation-2"}}  # Different thread_id!
)
print(f"\n--- New Conversation ---")
print(f"User: Do you know my name?")
print(f"Assistant: {response4['messages'][-1].content}")
```

**Why this is the NEW way (as of 2025)**:
- ✅ **LangGraph** replaces old memory classes (ConversationChain is deprecated)
- ✅ **MemorySaver** automatically persists conversation history
- ✅ **thread_id** allows multiple separate conversations
- ✅ More flexible and powerful than old memory systems
- ✅ Built for production applications with multiple users

---

### Example 5: Question-Answering Over Documents
**What it does**: Ask questions about your own documents

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits long text
from langchain_chroma import Chroma  # Vector database for storing embeddings
from langchain.chains import RetrievalQA  # Chain for question-answering
from dotenv import load_dotenv

load_dotenv()

# Sample document text (in real use, load from file)
document_text = """
LangChain is a framework for developing applications powered by language models.
It was created by Harrison Chase in 2022.
LangChain provides tools for prompt management, memory, and integration with external data.
The framework supports both Python and JavaScript.
Common use cases include chatbots, document analysis, and automated workflows.
"""

# Step 1: Split the document into chunks
# Why? LLMs have token limits, so we split long documents into smaller pieces
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,  # Each chunk will be ~200 characters
    chunk_overlap=20  # 20 characters overlap between chunks (maintains context)
)
chunks = text_splitter.split_text(document_text)

# Step 2: Create embeddings
# Embeddings convert text into numbers (vectors) that capture meaning
embeddings = OpenAIEmbeddings()

# Step 3: Store chunks in a vector database
# Chroma is a vector database that stores embeddings for fast similarity search
vectorstore = Chroma.from_texts(
    texts=chunks,  # Our text chunks
    embedding=embeddings  # The embedding model to use
)

# Step 4: Create a retriever
# Retriever searches the vector database for relevant chunks
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 2}  # Return top 2 most relevant chunks
)

# Step 5: Initialize LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Step 6: Create QA chain
# RetrievalQA combines retrieval and question-answering
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,  # The language model
    chain_type="stuff",  # "stuff" means put all retrieved docs into prompt
    retriever=retriever  # Our retriever
)

# Step 7: Ask questions!
question = "Who created LangChain?"
answer = qa_chain.invoke(question)
print(f"Question: {question}")
print(f"Answer: {answer['result']}")

question2 = "What programming languages does LangChain support?"
answer2 = qa_chain.invoke(question2)
print(f"\nQuestion: {question2}")
print(f"Answer: {answer2['result']}")
```

**How it works**:
1. Document is split into chunks
2. Each chunk is converted to embeddings (vectors)
3. Embeddings are stored in a vector database
4. When you ask a question, it finds the most relevant chunks
5. Those chunks are given to the LLM along with your question
6. LLM generates an answer based on the relevant chunks

---

### Example 6: Loading Documents from Files
**What it does**: Read PDFs, text files, and answer questions

```python
from langchain_community.document_loaders import TextLoader  # For .txt files
from langchain_community.document_loaders import PyPDFLoader  # For .pdf files
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

load_dotenv()

# Load a text file
# Replace 'document.txt' with your file path
try:
    loader = TextLoader('document.txt')
    documents = loader.load()  # Returns a list of Document objects
    
    # For PDF files, use this instead:
    # loader = PyPDFLoader('document.pdf')
    # documents = loader.load()
    
    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    splits = text_splitter.split_documents(documents)
    
    # Create vector store
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings
    )
    
    # Create QA chain
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever()
    )
    
    # Ask questions
    result = qa.invoke("What is the main topic of this document?")
    print(result['result'])
    
except FileNotFoundError:
    print("File not found. Create a 'document.txt' file first!")
```

---

### Example 7: Simple Agent with Tools
**What it does**: An agent that can use tools (calculator, search)

```python
from langchain_openai import ChatOpenAI
from langchain.agents import Tool, initialize_agent, AgentType
from dotenv import load_dotenv

load_dotenv()

# Define custom tools
# A tool is a function the agent can use

# Tool 1: Simple calculator
def calculator(expression: str) -> str:
    """Evaluates a mathematical expression"""
    try:
        # eval() executes Python expressions (use carefully in production!)
        result = eval(expression)
        return f"The result is {result}"
    except Exception as e:
        return f"Error: {str(e)}"

# Tool 2: Text length counter
def text_length(text: str) -> str:
    """Returns the length of text"""
    return f"The text has {len(text)} characters"

# Create Tool objects
# Tools wrap our functions so the agent can use them
tools = [
    Tool(
        name="Calculator",  # Name the agent will see
        func=calculator,  # The function to call
        description="Useful for math calculations. Input should be a math expression like '5 + 3'"
    ),
    Tool(
        name="TextLength",
        func=text_length,
        description="Returns the number of characters in a text. Input should be the text."
    )
]

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Create an agent
# The agent can decide which tool to use based on the question
agent = initialize_agent(
    tools=tools,  # Available tools
    llm=llm,  # The language model
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Agent type (ReAct = Reasoning + Acting)
    verbose=True  # Show the agent's thinking process
)

# Ask the agent questions
print("=== Question 1 ===")
response1 = agent.invoke("What is 25 multiplied by 4?")
print(response1['output'])

print("\n=== Question 2 ===")
response2 = agent.invoke("How many characters are in 'LangChain is awesome'?")
print(response2['output'])

print("\n=== Question 3 ===")
response3 = agent.invoke("If I have 'Hello World' and it has X characters, what is X plus 10?")
print(response3['output'])
```

**How agents work**:
1. Agent receives your question
2. Agent reasons about which tool(s) to use
3. Agent calls the appropriate tool(s)
4. Agent uses the tool output to form a final answer

---

## Advanced Topics

### 1. **Advanced Memory: Message Trimming**
Limit context window by keeping only recent messages:

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, trim_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from dotenv import load_dotenv

load_dotenv()

# Initialize LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# Create a trimmer to keep only the last 2 messages
# This prevents context window from getting too large
trimmer = trim_messages(
    strategy="last",  # Keep the most recent messages
    max_tokens=2,  # Keep only 2 messages
    token_counter=len  # Count each message as 1 token
)

# Create StateGraph with trimming
workflow = StateGraph(state_schema=MessagesState)

def call_model(state: MessagesState):
    """Call model with trimmed message history"""
    # Trim the messages to only keep recent ones
    trimmed_messages = trimmer.invoke(state["messages"])
    
    system_prompt = "You are a helpful assistant."
    messages = [SystemMessage(content=system_prompt)] + trimmed_messages
    
    response = llm.invoke(messages)
    return {"messages": response}

workflow.add_node("model", call_model)
workflow.add_edge(START, "model")

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# Test: Give it multiple messages
chat_history = [
    HumanMessage(content="My name is Nemo"),
    HumanMessage(content="I live in Sydney"),
    HumanMessage(content="I like swimming"),
]

# Ask a question that requires early context
response = app.invoke(
    {"messages": chat_history + [HumanMessage(content="What's my name?")]},
    config={"configurable": {"thread_id": "trim-test"}}
)

print(response['messages'][-1].content)
# It won't remember "Nemo" because that message was trimmed!
```

**When to use**: When you have long conversations and want to reduce costs/improve performance

---

### 2. **Advanced Memory: Conversation Summary**
Automatically summarize old messages to save context:

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

workflow = StateGraph(state_schema=MessagesState)

def call_model(state: MessagesState):
    """
    If conversation gets too long (4+ messages), create a summary
    and replace old messages with the summary
    """
    system_prompt = (
        "You are a helpful assistant. "
        "The chat history includes a summary of earlier conversation."
    )
    system_message = SystemMessage(content=system_prompt)
    
    # Get all messages except the most recent user input
    message_history = state["messages"][:-1]
    
    # If history is getting long, summarize it
    if len(message_history) >= 4:
        last_human_message = state["messages"][-1]
        
        # Ask LLM to create a summary
        summary_prompt = (
            "Distill the above chat messages into a single summary message. "
            "Include as many specific details as you can."
        )
        summary_message = llm.invoke(
            message_history + [HumanMessage(content=summary_prompt)]
        )
        
        # Remove old messages
        delete_messages = [RemoveMessage(id=m.id) for m in state["messages"]]
        
        # Re-add the latest user message
        human_message = HumanMessage(content=last_human_message.content)
        
        # Get response with summary
        response = llm.invoke([system_message, summary_message, human_message])
        
        # Return: summary + user message + response + deletions
        return {
            "messages": [summary_message, human_message, response] + delete_messages
        }
    else:
        # Not enough messages yet, just respond normally
        response = llm.invoke([system_message] + state["messages"])
        return {"messages": response}

workflow.add_node("model", call_model)
workflow.add_edge(START, "model")

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# Have a long conversation
demo_history = [
    HumanMessage(content="Hey! My name is Nemo"),
    HumanMessage(content="I live in the ocean"),
    HumanMessage(content="I'm looking for my dad"),
    HumanMessage(content="His name is Marlin"),
]

response = app.invoke(
    {"messages": demo_history + [HumanMessage("What do you know about me?")]},
    config={"configurable": {"thread_id": "summary-test"}}
)

print(response['messages'][-1].content)
# The AI will remember key details from the summary!
```

**When to use**: For very long conversations where you want to retain important details but reduce token usage

---

### 3. **Streaming Responses**
Get responses word-by-word (like ChatGPT):

```python
from langchain_openai import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from dotenv import load_dotenv

load_dotenv()

# Initialize LLM with streaming
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    streaming=True,  # Enable streaming
    callbacks=[StreamingStdOutCallbackHandler()]  # Print tokens as they arrive
)

# This will print the response as it's generated
llm.invoke("Write a short poem about coding")
```

---

### 2. **Custom Output Parsing**
Structure the LLM's output:

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field  # For data validation
from dotenv import load_dotenv

load_dotenv()

# Define the structure we want
class Person(BaseModel):
    """Data model for a person"""
    name: str = Field(description="Person's name")
    age: int = Field(description="Person's age")
    occupation: str = Field(description="Person's occupation")

# Create parser
parser = PydanticOutputParser(pydantic_object=Person)

# Create prompt with format instructions
template = """Extract information about the person from the following text.
{format_instructions}

Text: {text}
"""

prompt = ChatPromptTemplate.from_template(template)

# Initialize LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Create chain
chain = prompt | llm | parser

# Run with format instructions
result = chain.invoke({
    "text": "John is a 30-year-old software engineer.",
    "format_instructions": parser.get_format_instructions()
})

# Result is now a Python object!
print(f"Name: {result.name}")
print(f"Age: {result.age}")
print(f"Occupation: {result.occupation}")
```

---

### 3. **RAG (Retrieval Augmented Generation) - Complete Example**
Build a document Q&A system with web loading:

```python
from langchain_community.document_loaders import WebBaseLoader  # Load web pages
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

load_dotenv()

# Load content from a website
loader = WebBaseLoader("https://en.wikipedia.org/wiki/Artificial_intelligence")
documents = loader.load()

# Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Larger chunks for web content
    chunk_overlap=100
)
splits = text_splitter.split_documents(documents)

# Create embeddings and vector store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./chroma_db"  # Save to disk
)

# Create retriever
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}  # Top 3 relevant chunks
)

# Initialize LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Create QA chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True  # Also return the source chunks
)

# Ask questions
result = qa.invoke("What is artificial intelligence?")
print(f"Answer: {result['result']}")
print(f"\nSources used: {len(result['source_documents'])} documents")
```

---

## Best Practices

### 1. **Always Use Virtual Environments**
Keeps dependencies isolated and manageable.

### 2. **Secure Your API Keys**
Never commit `.env` files to Git. Add to `.gitignore`:
```
.env
```

### 3. **Handle Errors Gracefully**
```python
try:
    response = llm.invoke(prompt)
except Exception as e:
    print(f"Error: {e}")
```

### 4. **Monitor Token Usage**
LLM calls cost money based on tokens used:
```python
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    response = llm.invoke("Hello!")
    print(f"Tokens used: {cb.total_tokens}")
    print(f"Cost: ${cb.total_cost}")
```

### 5. **Test with Small Models First**
Use `gpt-4o-mini` for testing, upgrade to `gpt-4o` for production.

### 6. **Chunk Size Matters**
- Too small: Loses context
- Too large: Exceeds token limits
- Sweet spot: 500-1000 characters for most documents

### 7. **Use Caching**
Avoid repeated API calls:
```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())
```

---

## Common Patterns

### Pattern 1: Sequential Chain
When one step depends on the previous:
```python
# Step 1: Generate a topic
# Step 2: Write about that topic
# Step 3: Summarize what was written
```

### Pattern 2: Parallel Processing
When steps are independent:
```python
# Analyze sentiment AND extract entities simultaneously
```

### Pattern 3: Conditional Logic
Choose different paths based on input:
```python
# If question is about math -> use calculator
# If question is about facts -> search web
# Otherwise -> use general LLM
```

---

## Next Steps

1. **Explore LangGraph**: Build more complex multi-agent systems
2. **Try Different Models**: Experiment with Claude, Llama, Gemini
3. **Build Projects**:
   - Personal document Q&A system
   - Customer support chatbot
   - Code documentation assistant
   - Research paper summarizer
4. **Learn LangSmith**: Monitor and debug your LangChain apps
5. **Join Community**: LangChain Discord, GitHub discussions

---

## Common Issues & Solutions

### Issue 1: "Module not found"
```bash
# Make sure virtual environment is activated
# Reinstall the package
pip install langchain-openai
```

### Issue 2: "API key not found"
```python
# Check if .env file is in the same directory
# Verify load_dotenv() is called before using LLM
from dotenv import load_dotenv
load_dotenv()
```

### Issue 3: "Rate limit exceeded"
- You're making too many requests
- Wait a minute and try again
- Consider implementing retry logic with exponential backoff

### Issue 4: "Context length exceeded"
- Your input is too long
- Use smaller chunk sizes
- Retrieve fewer documents (reduce `k` value)

---

## Resources

- **Official Docs**: [https://python.langchain.com](https://python.langchain.com)
- **GitHub**: [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)
- **Discord Community**: Active community for help
- **LangSmith**: [https://smith.langchain.com](https://smith.langchain.com) for monitoring

---

## Glossary

- **LLM**: Large Language Model (e.g., GPT-4, Claude)
- **Prompt**: Instructions given to an LLM
- **Chain**: Sequence of operations
- **Agent**: Autonomous system that uses tools
- **Embedding**: Vector representation of text
- **Vector Store**: Database for storing embeddings
- **RAG**: Retrieval Augmented Generation (Q&A with your documents)
- **Token**: Basic unit of text for LLMs (roughly 4 characters)

---

**Congratulations!** 🎉 You now have a solid foundation in LangChain. Start building, experiment, and don't be afraid to make mistakes – that's how you learn!
